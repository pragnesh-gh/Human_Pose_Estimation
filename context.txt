def get_pose3D(video_path, output_dir, is_image=False, args=None):
    # args, _ = argparse.ArgumentParser().parse_known_args()
    _dbg("Entering get_pose3D", video_path=video_path, output_dir=output_dir, is_image=is_image)

    if args is None:
        class Dummy: pass
        args = Dummy()
    args.embed_dim_ratio, args.depth, args.frames = 32, 4, 243
    args.number_of_kept_frames, args.number_of_kept_coeffs = 27, 27
    args.pad = (args.frames - 1) // 2
    args.previous_dir = 'checkpoint/'
    args.n_joints, args.out_joints = 17, 17

    ## Reload
    model = nn.DataParallel(Model(args=args)).cuda()
    # model_dict = model.state_dict()
    # Put the pretrained model of PoseFormerV2 in 'checkpoint/']
    model_path = sorted(glob.glob(os.path.join(args.previous_dir, '27_243_45.2.bin')))[0]
    _dbg("Model & checkpoint", model_path=model_path)
    pre_dict = torch.load(model_path)
    model.load_state_dict(pre_dict['model_pos'], strict=True)

    model.eval()

    ## input
    # --- NEW: also load valid_mask (frame-aligned presence) ---
    _npz = np.load(output_dir + 'input_2D/keypoints.npz', allow_pickle=True)
    keypoints = _npz['reconstruction']
    valid_mask = _npz['valid_mask'] if 'valid_mask' in _npz.files else None  # 1=person present

    # source handling
    if is_image:
        video_length = 1
        img0 = cv2.imread(video_path)
        if img0 is None:
            raise FileNotFoundError(f"Cannot read image: {video_path}")
    else:
        cap = cv2.VideoCapture(video_path)
        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if POSE_DEBUG:
        kp_T = keypoints.shape[1]
        print(f"[3D.DBG] video_len={video_length}, keypoints_T={kp_T}, "
              f"valid_mask_len={(len(valid_mask) if valid_mask is not None else 'None')}")

    # --- NEW: cache base frame size & last good frame for robust timeline ---
    if not is_image:
        base_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 0
        base_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 0
    else:
        base_h, base_w = img0.shape[:2]
    if base_w <= 0 or base_h <= 0:
        base_w, base_h = 640, 480  # safe fallback
    # last_valid_img = None

    # --- Orientation buffers (only used if enabled) ---
    orientation_rows = []
    prev_quat = None
    prev_post_out = None # remember previous 3D pose to hold-through gaps

    ## 3D
    print('\nGenerating 3D pose...')
    for i in tqdm(range(video_length)):
        # Getting the frame
        if is_image:
            img = img0
        else:
            ret, img = cap.read()
            if not ret or img is None:
                if POSE_DEBUG:
                    print(f"[3D.DBG] Frame {i}: cap.read() failed")
                # still produce empty outputs to keep timeline
                img = np.zeros((base_h, base_w, 3), dtype=np.uint8)
        # img_size = img.shape

        # --- decide if this frame has a person (avoid “speed-up” on gaps) ---
        has_person = True
        if valid_mask is not None and i < len(valid_mask):
            has_person = bool(valid_mask[i])
        elif valid_mask is not None and i >= len(valid_mask):
            # if keypoints shorter than video (should not happen after patch), treat as no person
            has_person = False
        # --- NEW END ---

        ## input frames
        start = max(0, i - args.pad)
        end = min(i + args.pad, len(keypoints[0]) - 1)

        input_2D_no = keypoints[0][start:end + 1]

        left_pad, right_pad = 0, 0
        if input_2D_no.shape[0] != args.frames:
            if i < args.pad:
                left_pad = args.pad - i
            if i > len(keypoints[0]) - args.pad - 1:
                right_pad = i + args.pad - (len(keypoints[0]) - 1)

            input_2D_no = np.pad(input_2D_no, ((left_pad, right_pad), (0, 0), (0, 0)), 'edge')

        if POSE_DEBUG:
            print(f"[3D.DBG] Frame {i}: has_person={has_person} win=[{start},{end}] padL={left_pad} padR={right_pad}")



        # --- NEW: if no person, skip model forward; save raw 2D and held 3D ---
        if not has_person:
            # Save plain 2D frame (no overlay)
            output_dir_2D = output_dir + 'pose2D/'
            os.makedirs(output_dir_2D, exist_ok=True)
            cv2.imwrite(output_dir_2D + f"{i:04d}_2D.png", img)

            # Hold last 3D pose or zeros
            post_out = prev_post_out if prev_post_out is not None else np.zeros((17, 3), dtype=np.float32)
            fig = plt.figure(figsize=(9.6, 5.4))
            gs = gridspec.GridSpec(1, 1);
            gs.update(wspace=-0.00, hspace=0.05)
            ax = plt.subplot(gs[0], projection='3d')
            show3Dpose(post_out, ax)
            output_dir_3D = output_dir + 'pose3D/'
            os.makedirs(output_dir_3D, exist_ok=True)
            plt.savefig(output_dir_3D + f"{i:04d}_3D.png", dpi=200, format='png', bbox_inches='tight')
            plt.clf();
            plt.close(fig)
            continue
        # Center frame's 2D in pixels for overlay
        overlay_2D_px = input_2D_no[args.pad]

        # input_2D_no += np.random.normal(loc=0.0, scale=5, size=input_2D_no.shape)
        input_2D = normalize_screen_coordinates(input_2D_no, w=img.shape[1], h=img.shape[0])
        input_2D_aug = copy.deepcopy(input_2D)
        input_2D_aug[:, :, 0] *= -1
        joints_left = [4, 5, 6, 11, 12, 13]
        joints_right = [1, 2, 3, 14, 15, 16]
        input_2D_aug[:, :, 0] *= -1
        input_2D_aug[:, joints_left + joints_right] = input_2D_aug[:, joints_right + joints_left]
        input_2D = np.concatenate((np.expand_dims(input_2D, axis=0), np.expand_dims(input_2D_aug, axis=0)), 0)
        # (2, 243, 17, 2)

        input_2D = input_2D[np.newaxis, :, :, :, :]

        input_2D = torch.from_numpy(input_2D.astype('float32')).cuda()

        _dbg("Loaded input_2D", npz_path=output_dir + 'input_2D/keypoints.npz',
             shape=np.load(output_dir + 'input_2D/keypoints.npz', allow_pickle=True)['reconstruction'].shape)

        # N = input_2D.size(0)

        ## estimation
        output_3D_non_flip = model(input_2D[:, 0])
        output_3D_flip = model(input_2D[:, 1])
        # [1, 1, 17, 3]

        output_3D_flip[:, :, :, 0] *= -1
        output_3D_flip[:, :, joints_left + joints_right, :] = output_3D_flip[:, :, joints_right + joints_left, :]

        output_3D = (output_3D_non_flip + output_3D_flip) / 2

        output_3D[:, :, 0, :] = 0
        post_out = output_3D[0, 0].cpu().detach().numpy()
        prev_post_out = post_out #remember for gap frames

        rot = np.array([0.1407056450843811, -0.1500701755285263, -0.755240797996521, 0.6223280429840088],dtype='float32')
        post_out = camera_to_world(post_out, R=rot, t=0)
        post_out[:, 2] -= np.min(post_out[:, 2])

        # input_2D_no = input_2D_no[args.pad]

        overlay_src_idx = min(i, keypoints.shape[1] - 1)
        overlay_2D_px = keypoints[0, overlay_src_idx]  # shape (17, 2)

        # Frame timing introspection from OpenCV
        fps = float(cap.get(cv2.CAP_PROP_FPS)) if not is_image else 0.0
        pos_ms = float(cap.get(cv2.CAP_PROP_POS_MSEC)) if not is_image else 0.0
        est_ms = (i / fps * 1000.0) if (not is_image and fps > 0) else 0.0

        if POSE_DEBUG:
            print(
                "[SYNC.DBG] "
                f"i={i} start={start} end={end} padL={left_pad} padR={right_pad} "
                f"overlay_src_idx={overlay_src_idx} "
                f"pos_ms={pos_ms:.2f} est_ms={est_ms:.2f} fps={fps:.2f}"
            )

        ## 2D
        image = show2Dpose(overlay_2D_px, copy.deepcopy(img))
        cv2.putText(image, f"frame={i}", (12, 32), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2)

        output_dir_2D = output_dir + 'pose2D/'
        os.makedirs(output_dir_2D, exist_ok=True)
        cv2.imwrite(output_dir_2D + f"{i:04d}_2D.png", image)

        ## 3D
        fig = plt.figure(figsize=(9.6, 5.4))
        gs = gridspec.GridSpec(1, 1)
        gs.update(wspace=-0.00, hspace=0.05)
        ax = plt.subplot(gs[0], projection='3d')
        show3Dpose(post_out, ax)

        # --- Optional: compute torso orientation & overlay triad ---
        if getattr(args, "estimate_orientation", False):
            layout = getattr(args, "orientation_layout",
                             "h36m")  # Default H36M; for COCO-17 pass --orientation-layout coco17
            alpha = float(getattr(args, "orientation_alpha", 0.6))
            conf_min = float(getattr(args, "orientation_conf_min", 0.5))

            # Compute torso frame (camera/world consistent with post_out)
            quat_wxyz, fwd, right, up, conf = compute_torso_frame(post_out, layout=layout)
            conf = 0.0 if conf is None else float(conf)

            # Confidence-gated SLERP-EMA
            cur_quat = quat_wxyz
            if conf < conf_min and prev_quat is not None:
                smoothed = prev_quat
            else:
                smoothed = smooth_quat(prev_quat, cur_quat, alpha) if prev_quat is not None else cur_quat
            prev_quat = smoothed

            # Persist a tidy row for Colab
            orientation_rows.append({
                "frame_index": int(i),
                "quat_w": float(smoothed[0]),
                "quat_x": float(smoothed[1]),
                "quat_y": float(smoothed[2]),
                "quat_z": float(smoothed[3]),
                "confidence": conf,
                "forward_x": float(fwd[0]), "forward_y": float(fwd[1]), "forward_z": float(fwd[2]),
                "right_x": float(right[0]), "right_y": float(right[1]), "right_z": float(right[2]),
                "up_x": float(up[0]), "up_y": float(up[1]), "up_z": float(up[2]),
            })

            # Optional overlay on the same 3D axes
            if getattr(args, "orientation_overlay", False):
                # Anchor at pelvis/root. For COCO-17, change root index accordingly.
                root_idx = 0  # H36M pelvis
                origin = post_out[root_idx]  # [3]

                # Triad length relative to the scene size
                radius = max(1e-6, (post_out.max(axis=0) - post_out.min(axis=0)).max() / 2.0)
                s = float(getattr(args, "orientation_overlay_scale", 1.0)) * radius * 0.2

                # Draw F (red), R (green), U (blue)
                ax.plot([origin[0], origin[0] + s * fwd[0]],
                        [origin[1], origin[1] + s * fwd[1]],
                        [origin[2], origin[2] + s * fwd[2]], lw=2, c='r')
                ax.plot([origin[0], origin[0] + s * right[0]],
                        [origin[1], origin[1] + s * right[1]],
                        [origin[2], origin[2] + s * right[2]], lw=2, c='g')
                ax.plot([origin[0], origin[0] + s * up[0]],
                        [origin[1], origin[1] + s * up[1]],
                        [origin[2], origin[2] + s * up[2]], lw=2, c='b')

                # add legend once per figure
                ax.plot([], [], c='r', label='Forward (Z)')
                ax.plot([], [], c='g', label='Right (X)')
                ax.plot([], [], c='b', label='Up (Y)')
                ax.legend(loc='upper right')

        output_dir_3D = output_dir + 'pose3D/'
        os.makedirs(output_dir_3D, exist_ok=True)
        plt.savefig(output_dir_3D + str(('%04d' % i)) + '_3D.png', dpi=200, format='png', bbox_inches='tight')
        plt.clf()
        plt.close(fig)

    print('Generating 3D pose successful!')

    if not is_image:
        cap.release()

    # --- Optional: save per-frame orientations for Colab ---
    if getattr(args, "estimate_orientation", False) and getattr(args, "orientation_save", None):
        out_path = Path(args.orientation_save)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        ext = out_path.suffix.lower()
        if ext == ".csv":
            with out_path.open("w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(
                    f,
                    fieldnames=[
                        "frame_index",
                        "quat_w", "quat_x", "quat_y", "quat_z",
                        "confidence",
                        "forward_x", "forward_y", "forward_z",
                        "right_x", "right_y", "right_z",
                        "up_x", "up_y", "up_z",
                    ]
                )
                writer.writeheader()
                writer.writerows(orientation_rows)
            print(f"[orientation] Saved CSV: {out_path}")
        else:
            p = out_path if ext == ".json" else out_path.with_suffix(".json")
            with p.open("w", encoding="utf-8") as f:
                json.dump(orientation_rows, f, ensure_ascii=False)
            print(f"[orientation] Saved JSON: {p}")

    ## all
    image_dir = 'results/'
    image_2d_dir = sorted(glob.glob(os.path.join(output_dir_2D, '*.png')))
    image_3d_dir = sorted(glob.glob(os.path.join(output_dir_3D, '*.png')))

    if POSE_DEBUG:
        print(f"[COMPOSE.DBG] pose2D_frames={len(image_2d_dir)} pose3D_frames={len(image_3d_dir)}")

    print('\nGenerating demo...')
    for i in tqdm(range(len(image_2d_dir))):
        image_2d = plt.imread(image_2d_dir[i])
        image_3d = plt.imread(image_3d_dir[i])

        # ## crop
        # edge = (image_2d.shape[1] - image_2d.shape[0]) // 2
        # image_2d = image_2d[:, edge:image_2d.shape[1] - edge]
        #
        # edge = 130
        # image_3d = image_3d[edge:image_3d.shape[0] - edge, edge:image_3d.shape[1] - edge]

        ## show
        font_size = 12
        fig = plt.figure(figsize=(15.0, 5.4))
        ax = plt.subplot(121)
        showimage(ax, image_2d)
        ax.set_title("Input", fontsize=font_size)

        ax = plt.subplot(122)
        showimage(ax, image_3d)
        ax.set_title("Reconstruction", fontsize=font_size)

        ## save
        output_dir_pose = output_dir + 'pose/'
        os.makedirs(output_dir_pose, exist_ok=True)
        plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)
        plt.margins(0, 0)
        plt.savefig(output_dir_pose + str(('%04d' % i)) + '_pose.png', dpi=200, bbox_inches='tight')
        plt.clf()
        plt.close(fig)
        if (i % 50) == 0:
            plt.close('all')  # just in case

    if POSE_DEBUG:
        print(f"[COMPOSE.DBG] wrote {len(image_2d_dir)} composite frames to {output_dir + 'pose/'}")