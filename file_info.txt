Project Structure:

Project Root: C:\Users\ppk_2\Desktop\University\SummerSem-25\Human_Pose_Estimate\PoseFormerV2_latest

Repository Structure and Important Files:

checkpoint/
  └── 27_243_45.2.bin

common/
  ├── arguments.py
  ├── camera.py
  ├── custom_dataset.py
  ├── generators.py
  ├── h36m_dataset.py
  ├── loss.py
  ├── mocap_dataset.py
  ├── model_poseformer.py
  ├── orientation.py
  ├── quaternion.py
  ├── skeleton.py
  ├── utils.py
  ├── visualization.py
  └── README.MD

demo/
  ├── image/
  ├── lib/
  ├── output/
  ├── video/
  │   ├── pose_pipeline.py
  │   ├── pose_pipeline_singleFrame.py
  │   └── vis.py
  └── yolov8n.pt

images/

mpi_inf_3dhp/

run_poseformer.py

single_frame.ipynb

status_update.txt

vectors_and_angles.csv

yolov8n.pt

file_info.txt

plan.txt

README.md

requirements.txt

LICENSE




Descriptions:

From the GitHub repo (PoseFormerV2):
Project focus: 3D human pose estimation from 2D keypoints.

Based on transformers — PoseFormerV2 is an improved architecture over PoseFormer (with better spatio-temporal modeling).

Benchmarks: Human3.6M, MPI-INF-3DHP datasets.

Reported to outperform many baselines in accuracy and efficiency.

Provides:

Pretrained weights and checkpoints.

Training/evaluation scripts (like the one you uploaded).

Configurable arguments (dataset choice, keypoints type, number of frames, downsampling, etc.).

Visualization tools for qualitative results.

Repo structure:

common/ → utilities (datasets, models, losses, camera tools, visualization).

data/ → expected dataset storage.

checkpoints/ → pretrained model weights.

Main scripts: run_poseformer.py (training/eval) and helpers.

run_poseformer.py:
It’s the main training and evaluation script for PoseFormerV2.

Imports from the repo’s common modules: arguments parsing, camera utilities, datasets (H36M or custom), model definition (PoseTransformerV2), loss functions, generators, visualization, etc.

Defines get_poseformer_model() — a helper that lets you load a trained model + args safely (useful for notebooks or pipelines).

Main functionalities:

Loads dataset (default Human3.6M or custom).

Prepares 2D detections and aligns with 3D ground truth.

Builds and initializes the PoseFormerV2 model (transformer-based).

Training loop: uses AdamW, supports resuming checkpoints, saves best checkpoints.

Evaluation: computes MPJPE, P-MPJPE, N-MPJPE, velocity errors, and (if MPI dataset) PCK/AUC.

Supports visualization (render_animation) to compare predictions vs. ground truth.

Includes checkpoint management (best_epoch.bin, epoch_X.bin) and logging.

GPU support: wraps models in nn.DataParallel if CUDA is available.

The script has been slightly modified (there’s a patch note at the top about preventing CLI parsing on import).


vis.py:
It is the main demo script for PoseFormerV2 that takes either a video, a single image, or a folder of images, generates 2D keypoints with HRNet (using a sys.argv strip hack to avoid HRNet’s argparse conflicts), lifts them to 3D with the pretrained PoseFormerV2 checkpoint (27_243_45.2.bin), and produces a side-by-side visualization showing the input (with 2D overlay) and the reconstructed 3D skeleton. The pipeline is: get_pose2D → save input_2D/keypoints.npz, get_pose3D → normalize, augment, run the transformer, and render PNGs (pose2D/, pose3D/, pose/), and finally combine results. For video input it stitches frames into an MP4, for a single image it saves one PNG, and for a folder it repeats the image flow for each file. Key arguments are --video, --image, --image-dir, and --gpu. The script ensures only one parser is used, fixes cropping so subjects are not cut, and handles both absolute paths and default ./demo/video/ or ./demo/image/ locations.
Still runs image/video/folder demo with its own argparse group; orientation is currently wired via the global args only when you integrate it into the main pipeline scripts. (No conflict; separate parser.)
arguments.py - all the command-line flags in one place:
Defines every CLI option you can pass when running training/eval/visualization: which dataset, which 2D keypoints, which subjects to train/test, checkpoint paths, whether to render, GPU selection, learning rate, epochs, batch size, number of frames, transformer depth, etc. It also has a bunch of visualization options (subject/action/camera/video, output path, bitrate, frame limits). It validates a couple of incompatible combos (e.g., you can’t use --resume and --evaluate together).
Every top-level script (like run_poseformer.py or your demo/vis.py) calls parse_args() here to get a single args object. All downstream modules (data loading, model build, training loop, visualization) read from args to stay consistent. For example, --number-of-frames decides the model’s temporal receptive field; --gpu controls CUDA_VISIBLE_DEVICES; --viz-* options steer rendering.
Added opt-in flags to enable orientation estimation and control layout/smoothing/overlay/output:
--estimate-orientation, --orientation-layout {h36m,coco17}, --orientation-alpha, --orientation-conf-min, --orientation-overlay, --orientation-overlay-scale, --orientation-save {csv|json}.
Defaults keep existing behavior unchanged

camera.py — coordinate transforms + projection math:
What’s inside (plain English):

Utility functions for converting coordinates between different spaces and for normalizing/un-normalizing 2D keypoints:

normalize_screen_coordinates(X, w, h): maps image-pixel 2D points to a normalized range roughly in [-1, 1] while keeping aspect ratio. Used before feeding 2D into the model.

image_coordinates(X, w, h): inverts the above — takes normalized 2D and brings it back to image pixel coordinates (handy for visualization).

world_to_camera(X, R, t) and camera_to_world(X, R, t): rotate/translate 3D points between world and camera frames using quaternions.

project_to_2d(...) and project_to_2d_linear(...): project 3D camera-space points onto the image plane with Human3.6M intrinsics; the first includes radial/tangential distortion, the second uses only focal length & principal point.

Relies on helpers from common.utils.wrap and quaternion ops qrot/qinverse

How it ties into the bigger picture:

Preprocessing: 2D detections are normalized per camera before entering PoseFormerV2 (so the model sees a consistent scale across videos).

Label prep: Ground-truth 3D is converted between world/camera coordinates; global trajectory can be removed/added as needed.

Evaluation & viz: After predicting 3D, you may re-apply trajectory, convert camera→world, and/or project back to 2D for overlays or metric computations that need camera geometry. These steps are crucial for fair, apples-to-apples metrics like MPJPE and for producing clean renderings

custom_dataset.py:
What I see

Defines a CustomDataset class that inherits from MocapDataset. It’s meant for running PoseFormerV2 on your own 2D detections (not Human3.6M).

Loads a .npz file of detections (expects metadata.video_metadata for per-video width/height).

Builds a per-video camera dict (resolution from metadata + fixed “dummy” extrinsics borrowed from H36M, just to make visualization work).

Optionally reduces the skeleton from 32→17 joints and fixes shoulder parent links—this matches the training/eval joint set PoseFormerV2 expects.

How it fits the bigger picture

When you run with a dataset name starting with custom..., PoseFormerV2 uses this class to create the internal structure the rest of the code expects: subjects/videos → cameras → 2D keypoints (+ optional 3D if you had it).

The “dummy” camera parameters allow you to normalize 2D and render predictions even if your real extrinsics are unknown. This keeps the model I/O format consistent with H36M

generators.py:
What I see

ChunkedGenerator: the training-time batcher.

Slices long sequences into fixed-length chunks (with padding) matching the model’s temporal window (receptive field).

Supports optional horizontal flip augmentation for 2D and 3D (and flips camera distortion signs).

Yields mini-batches of (cameras?, 3D?, 2D) depending on supervision mode.

UnchunkedGenerator: the testing/visualization-time iterator.

Returns full sequences (batch size 1), with appropriate padding and optional mirrored copies if augmentation is enabled.

Both ensure left/right keypoint & joint indices are swapped correctly during flips.

How it fits the bigger picture

These generators are the bridge between raw arrays and the model: they prepare exactly the windows the transformer expects (e.g., 27/81 frames), keep temporal context aligned, and handle padding so edge frames still produce outputs.

During evaluation, UnchunkedGenerator lets the code run TTA (flip + average) cleanly, which improves MPJPE metrics a bit without retraining

h36m_dataset.py:
What I see

Declares the Human3.6M skeleton (parents + left/right joint lists).

Hard-codes intrinsic params for the 4 H36M cameras and extrinsic (orientation quaternion + translation) per subject.

The Human36mDataset class:

Loads 3D poses from data_3d_h36m.npz into the internal dict structure.

Normalizes camera intrinsics to the coordinate system used across the repo (centers scaled to ~[-1, 1], focal scaled by image width, translations mm→meters).

Builds an “intrinsic vector” with focal, center, distortion, and a 3×3 projection-like set flattened—handy for downstream math.

Optionally reduces to 17 joints and fixes shoulder parents to match the common 17-joint layout.

Indicates it supports semi-supervised setups.
How it fits the bigger picture

This is the canonical benchmark dataset used to train/evaluate PoseFormerV2 (and to report MPJPE/P-MPJPE/N-MPJPE).

The consistent camera normalization here is why the rest of the code can safely do things like normalize 2D, convert world↔camera, and render with correct geometry for H36M.

Its structure (subjects → actions → cameras & positions) is mirrored by the CustomDataset, which keeps the whole codebase modular and dataset-agnostic.

How they work together (quick map)

Choose dataset via args → Human36mDataset for H36M or CustomDataset for your own 2D.

Generators take those arrays (2D, optional 3D, camera vectors) and deliver the right windowed batches to PoseFormerV2 during training/eval, including flip augmentation.

The model + losses run on these batches; evaluation/visualization rely on the same camera normalization and skeleton conventions defined in the dataset classes.

quaternion.py:
What’s inside (plain English):

qrot(q, v): rotates 3-D vectors v by quaternion(s) q. Works in batch, assumes q is shaped (*, 4) and v is (*, 3). Uses two cross-products to apply the rotation.

qinverse(q, inplace=False): returns the inverse of a unit quaternion by negating its vector part; supports in-place.
In addition to existing qrot and qinverse, now provides:
quat_normalize, quat_dot, quat_slerp, quat_from_rotmat, rotmat_from_quat, basis_from_quat.
Torch-native, batch-safe, wxyz convention. Used by orientation.py and available to camera/visualization utilities.


How it ties into the bigger picture:

These are the low-level math ops used by camera utilities (e.g., world↔camera transforms) when preparing data and during visualization/evaluation. Without correct quaternion rotation/inversion, 3D joints can’t be reliably moved between coordinate frames. This keeps geometry consistent across training, testing, and rendering.

model_poseformer.py (PoseFormerV2):
What’s inside (plain English):

Building blocks:

Mlp: 2-layer feed-forward with GELU + dropout (standard transformer MLP).

FreqMlp: same idea, but runs in the temporal frequency domain—applies DCT → MLP → iDCT to model temporal patterns.

Attention: multi-head self-attention (QKV projection, scaled dot-product, dropout).

Block: norm → attention → residual + norm → MLP → residual (standard ViT block).

MixedBlock: splits the sequence along time into two halves:

first half → standard Mlp,

second half → FreqMlp (DCT/iDCT path),
then concatenates back—mixing time-domain and frequency-domain processing.

Uses DropPath (stochastic depth) for regularization.

PoseTransformerV2 (the model):

Inputs: a clip of f frames × p joints × 2 channels (x,y).

Spatial embedding: per-joint linear Joint_embedding (2→embed_dim_ratio) + learned spatial positional embedding; passes through a stack of spatial Blocks and flattens across joints.

Temporal/frequency embedding: applies DCT across time, keeps only num_coeff_kept coefficients, linears to embed_dim, adds learned temporal positional embeddings.

Fusion: concatenates the frequency branch with the spatial feature sequence, then runs a stack of MixedBlocks (half time-domain, half frequency-domain MLP).

Aggregation: “weighted mean” via 1×1 Conv over kept coefficients and kept frames (two small convs) to compress the temporal dimension.

Head: LayerNorm → Linear → reshape to (B, 1, joints, 3) (predicts 3-D xyz per joint).

Model sizes and depths are driven by CLI args like embed_dim_ratio, depth, number_of_kept_frames, number_of_kept_coeffs.

How it ties into the bigger picture:

This is the core network that turns normalized 2-D keypoint tracks into 3-D joint coordinates.

It explicitly separates and then fuses spatial structure (relations across joints) and temporal structure (both in time and frequency), which is why PoseFormerV2 is strong on motion consistency.

Upstream pieces (datasets, camera transforms, generators) deliver correctly shaped, normalized windows; downstream pieces (losses/metrics, visualization) evaluate and display the 3-D outputs. The CLI args (from arguments.py) configure these internal sizes and depths so training/eval scripts can reuse the same model cleanly.

skeleton.py:
What it contains (simple):

A Skeleton class that stores the kinematic tree: parents array, and lists of left/right joints.

Utilities to query structure (num_joints, parents, children, has_children) and to remove joints safely while fixing indices and left/right groupings. It also precomputes children lists for fast traversal.

How it fits in:

The skeleton definition is the canonical joint graph used across training, evaluation, and visualization.

Rendering uses parents()/left-right splits for coloring and drawing bone lines; metrics and flips also depend on consistent left/right joint indexing.

utils.py:
What it contains (simple):

wrap(func, *args, unsqueeze=False): lets you call a PyTorch function using NumPy arrays; converts inputs to tensors and outputs back to NumPy—handy glue code.

deterministic_random(min_value, max_value, data): SHA-256–based pseudo-random integer for repeatable sampling/slicing (same input → same selection).

load_pretrained_weights(model, checkpoint): loads weights into a non-DataParallel model, ignoring mismatched keys/shapes and stripping any module. prefixes from saved state dicts. Prints how many layers matched.

How it fits in:

wrap is used by camera/geo helpers so they can accept NumPy without boilerplate.

deterministic_random ensures reproducible subset or window picks when downsampling.

load_pretrained_weights is a robust loader for checkpoints across different wrapping (DP vs non-DP) or minor architecture diffs.

visualization.py:
What it contains (simple):

A Matplotlib/FFmpeg-based renderer for side-by-side 2D input and 3D pose animation.

Helpers to probe video metadata via ffprobe (get_resolution, get_fps) and to stream frames with ffmpeg (read_video).

render_animation(...): builds a figure with the input frame and one or more 3D subplots (e.g., “Reconstruction”, “Ground truth”), colors right-side joints red, draws bones using the skeleton parents(), handles downsampling, TTA-compatible sequence alignment, and writes .mp4 (ffmpeg) or .gif (imagemagick). Uses trajectory to keep the camera centered on the subject per frame.

How it fits in:

This is the final qualitative output stage. After the model predicts 3D in camera/world coordinates, the training/eval script passes normalized 2D keypoints, 3D predictions (and optional GT), the Skeleton, FPS, and viewport to render_animation to create the comparison video. It relies on the same skeleton structure and camera normalization used in preprocessing.

Big-picture wiring (at a glance)

Datasets provide joints & cameras → generators window the sequences → model predicts 3D → loss/metrics score them → visualization draws them using the skeleton parents and left/right indices.

Utils keep conversions clean (NumPy↔Torch), selections reproducible, and checkpoint loading resilient.

