üìë Report: Adding Torso Orientation Estimation to PoseFormerV2
üîπ Current Pipeline (baseline understanding)

PoseFormerV2 currently:

Takes videos/clips as input (via .mp4 or preprocessed .npz).

Runs on sliding windows of 2D keypoints ‚Üí predicts 3D pose per frame.

Stitches predictions together for evaluation or visualization (visualization.py).

Outputs are currently 3D joints only (per frame, in camera or world coords).

No built-in notion of torso orientation / heading.

üîπ Project Goal (Part 1 focus)

Compute a robust torso orientation for each frame:

Represented as a quaternion (w, x, y, z).

Also derive and optionally visualize the forward vector for intuitive viewing.

Must handle occlusion, partial body, or seated/crossed-leg poses.

Use camera frame orientation only (to match depth sensor frame).

Orientation anchored at the mid-shoulder (fallback: pelvis/hips).

Provide temporal smoothing for stability.

Save per-frame outputs (quat + forward vector + confidence).

üîπ Orientation Estimation Algorithm (geometric, no retrain)

Compute anchor joints:

LS, RS, LH, RH, Neck, Head.

Midpoints: MidShoulder = (LS+RS)/2, MidHip = (LH+RH)/2.

Candidate torso axes:

Right axis candidates:

x_s = normalize(RS ‚Äì LS) (shoulders)

x_h = normalize(RH ‚Äì LH) (hips, fallback)

Up axis: y = normalize(MidShoulder ‚Äì MidHip) (spine).

Robust fusion:

Weighted sum: x = normalize(w_s¬∑x_s + w_h¬∑x_h)

Weights based on confidence, bone length consistency, or bbox detection quality.

Forward axis:

z = normalize(x √ó y) (cross product).

Re-orthonormalize: y = normalize(z √ó x) (Gram‚ÄìSchmidt).

Sign disambiguation:

Project (Neck‚ÜíHead) onto z.

If negative, flip z and recompute y.

Quaternion:

Convert [x y z] rotation matrix ‚Üí quaternion (w, x, y, z).

Store along with forward = z.

Temporal smoothing:

Default SLERP EMA with Œ±=0.2.

Comment: Œ± in [0.05‚Äì0.3];

lower = smoother (but laggy),

higher = snappier (but jittery).

üîπ Special Cases & Fallbacks

Partial body (hips down only):

Use pelvis‚Üíknee vector as fallback forward estimate.

Mark as lower confidence.

Heavily smooth to avoid jitter.

Occlusion / unreliable joints:

Use reliability heuristics:

Bone length deviation from median.

Joint velocity spikes.

Bounding box confidence (YOLO detector output).

If very low ‚Üí hold last reliable orientation.

üîπ Files to Modify / Create
1. New file: common/orientation.py

Helpers to compute orientation per frame:

def compute_torso_frame(joints3d, conf=None, anchor="midshoulder", use_hips_fallback=True):
    """
    Inputs:
      joints3d : np.array or torch.Tensor [N_joints, 3]
      conf     : optional confidence array
    Returns:
      R (3x3 rotation matrix)
      quat (w, x, y, z quaternion)
      forward (3-vector)
      confidence (float 0-1)
    """


Other helpers:

smooth_quat(prev_q, q, alpha=0.2) ‚Üí SLERP-based EMA.

matrix_to_quaternion(R) (if missing).

Optionally add ‚Äúreliability‚Äù checks here.

2. arguments.py

Add flags:

--torso-orient (bool) ‚Üí enable orientation computation.

--orient-alpha (float, default 0.2) ‚Üí smoothing factor (with comment on range/effects).

--orient-anchor (str, default "midshoulder") ‚Üí anchor joint.

--orient-out (path) ‚Üí save JSON/NPY of {quat, forward, confidence}.

--orient-frame (choice: camera | world; default camera).

3. run_poseformer.py

Hook orientation computation after predictions:

For each frame, call compute_torso_frame.

Apply smoothing (keep prev_q).

Save orientation outputs if --orient-out set.

Pass orientations to viz module if enabled.

4. visualization.py

Optional overlay of forward direction:

If --torso-orient, draw an arrow from anchor joint along forward vector.

Optionally print yaw angle per frame.

5. Reuse / optional edits

quaternion.py (already has qrot, qinverse).

Add matrix_to_quaternion if not present.

camera.py: no changes needed, but can reuse its coordinate transforms if later converting to world frame.

human_detector.py / bbox.py: optionally feed detection confidence into orientation reliability.

üîπ Optional Extensions

Evaluation metric: On H36M, derive GT forward (same geometric rule from GT 3D) ‚Üí compute angular error per frame (deg).

Hybrid fallback: If torso cues missing, combine pelvis‚Üíknee forward estimate with history (low confidence).

Future real-time: Replace EMA with a constant-velocity Kalman filter on yaw/pitch/roll for smoother predictions during dropouts.

Multi-person (later): would require person tracking + multi-bbox support; not needed now.

üîπ Why Quaternions (not just vectors)?

A single vector defines one axis, but not full 3D orientation (no roll/pitch).

Heading can flip without secondary cues (front vs back).

Quaternions encode the full torso orientation (yaw, pitch, roll) in a compact, gimbal-free form.

Directly usable in ROS TF / RViz.

Easy to smooth (SLERP) and to compose with camera‚Üírobot extrinsics.

For visualization: we draw axes/arrows derived from quaternion, not the quaternion itself.

üîπ Output Format (per frame)

Each frame should yield:

{
  "frame_id": int,
  "quat": [w, x, y, z],
  "forward": [fx, fy, fz],
  "confidence": float
}


Stored as .json or .npy.

üîπ Summary Flow

Predict 3D pose (PoseFormerV2).

Call compute_torso_frame.

Smooth quaternion over time.

Save per-frame orientation (quat + forward + conf).

Optionally overlay arrow in visualization.

This augments PoseFormerV2 to provide per-frame torso orientation, robust to occlusion, directly in camera coordinates, with quaternion outputs suitable for robotics use.